{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_urls = []\n",
    "vPath = './chromedriver/chromedriver'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(executable_path=vPath)\n",
    "\n",
    "# just scrape everything that is related first\n",
    "url_lst = [\"https://www.mycareersfuture.sg/search?search=data%20analyst&page={}\", \n",
    "           \"https://www.mycareersfuture.sg/search?search=data%20scientist&page={}\", \n",
    "           \"https://www.mycareersfuture.sg/search?search=analytics&page={}\", \n",
    "           \"https://www.mycareersfuture.sg/search?search=data%20engineer&page={}\", \n",
    "           \"https://www.mycareersfuture.sg/search?search=machine%20learning&page={}\", \n",
    "           \"https://www.mycareersfuture.sg/search?search=business%20intelligence&page={}\",\n",
    "           \"https://www.mycareersfuture.sg/search?search=business%20analyst&page={}\", \n",
    "           \"https://www.mycareersfuture.sg/search?search=artificial%20intelligence&page={}\",\n",
    "           \"https://www.mycareersfuture.sg/search?search=ai&page={}\",\n",
    "           \"https://www.mycareersfuture.sg/search?search=data&page={}\" \n",
    "          ]\n",
    "           \n",
    "for u in url_lst:\n",
    "    i = 0\n",
    "    for i in range(0,30):\n",
    "        driver.get(u.format(i))\n",
    "        sleep(5) \n",
    "        html = BeautifulSoup(driver.page_source, 'lxml')\n",
    "        for job_card in html.find_all('div', id=lambda x: x and x.startswith('job-card-')):\n",
    "            job_urls.append('https://www.mycareersfuture.sg'+job_card.find('a')['href'])\n",
    "      \n",
    "    \n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(job_urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_urls2 = set(job_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save urls as csv\n",
    "pd.DataFrame(list(job_urls2)).to_csv('jobs_url.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_df = pd.read_csv('jobs_url.csv')\n",
    "job_ls = job_df.iloc[:,1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop to get details\n",
    "companies = []\n",
    "titles = []\n",
    "emp_type = []\n",
    "seniority = []\n",
    "category = []\n",
    "salaries = []\n",
    "responsibilities = []\n",
    "requirements = []\n",
    "failed_url = []\n",
    "\n",
    "# loop through to get all the needed info, takes a while...\n",
    "driver = webdriver.Chrome(executable_path=vPath)\n",
    "for ind, url_job in enumerate(job_ls): \n",
    "    try:\n",
    "        driver.get(url_job)\n",
    "        sleep(5)\n",
    "        html_job = driver.page_source\n",
    "        soup_job = BeautifulSoup(html_job, 'lxml')\n",
    "        companies.append(soup_job.find('p', {'name': 'company'}))\n",
    "        titles.append(soup_job.find('h1', {'id': 'job_title'}))\n",
    "        emp_type.append(soup_job.find('p', {'id': 'employment_type'}))\n",
    "        seniority.append(soup_job.find('p', {'id': 'seniority'}))\n",
    "        category.append(soup_job.find('p', {'id': 'job-categories'}))\n",
    "        salaries.append(soup_job.find('div', {'class': 'lh-solid'}))\n",
    "        responsibilities.append(soup_job.find('div',{'id':'job_description'}))\n",
    "        requirements.append(soup_job.find('div',{'id':'requirements'}))\n",
    "    except:\n",
    "        failed_url.append(url_job)\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = [companies, titles, emp_type, seniority, category, salaries, responsibilities, requirements]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = {}\n",
    "for ind, ls in enumerate(info):\n",
    "    clean[ind] = []\n",
    "    for x in ls:\n",
    "        try:\n",
    "            clean[ind].append(x.text)\n",
    "        except:\n",
    "            clean[ind].append(np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert dictionary into dataframe and rename columns\n",
    "df = pd.DataFrame(clean)\n",
    "df.columns = ['Company', 'Title', 'Emp_type', 'Seniority', 'Industry', 'Salary', 'Responsibility', 'Requirements']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "df.to_csv('cf_jobsdf.csv', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
